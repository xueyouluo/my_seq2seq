{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "import attention_wrapper\n",
    "import beam_search_decoder\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi What is your name?\n",
      "([13, 6, 8, 11, 40, 4, 1, 1, 1, 1], 6)\n",
      "Hi this is Jaemin.\n",
      "([0, 13, 32, 8, 19, 3, 2, 1, 1, 1, 1], 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'_GO_ live industrial ! , . _END_ _PAD_ _PAD_ _PAD_ _PAD_'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# maximum length of input and target sentences including paddings\n",
    "enc_sentence_length = 10\n",
    "dec_sentence_length = 10\n",
    "\n",
    "# Batch_size: 2\n",
    "input_batches = [\n",
    "    ['Hi What is your name?', 'Nice to meet you!'],\n",
    "    ['Which programming language do you use?', 'See you later.'],\n",
    "    ['Where do you live?', 'What is your major?'],\n",
    "    ['What do you want to drink?', 'What is your favorite beer?']]\n",
    "\n",
    "target_batches = [\n",
    "    ['Hi this is Jaemin.', 'Nice to meet you too!'],\n",
    "    ['I like Python.', 'Bye Bye.'],\n",
    "    ['I live in Seoul, South Korea.', 'I study industrial engineering.'],\n",
    "    ['Beer please!', 'Leffe brown!']]\n",
    "\n",
    "all_input_sentences = []\n",
    "for input_batch in input_batches:\n",
    "    all_input_sentences.extend(input_batch)\n",
    "    \n",
    "all_target_sentences = []\n",
    "for target_batch in target_batches:\n",
    "    all_target_sentences.extend(target_batch)\n",
    "    \n",
    "def tokenizer(sentence):\n",
    "    tokens = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "    return tokens\n",
    "\n",
    "_START_ = \"_GO_\"\n",
    "_PAD_ = \"_PAD_\"\n",
    "_END_ = \"_END_\"\n",
    "\n",
    "def build_vocab(sentences, max_vocab_size=None):\n",
    "    word_counter = Counter()\n",
    "    vocab = dict()\n",
    "    reverse_vocab = dict()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer(sentence)\n",
    "        word_counter.update(tokens)\n",
    "        \n",
    "    if max_vocab_size is None:\n",
    "        max_vocab_size = len(word_counter)\n",
    "    \n",
    "    vocab[_START_] = 0\n",
    "    vocab[_PAD_] = 1\n",
    "    vocab[_END_] = 2\n",
    "    vocab_idx = 3\n",
    "    for key, value in word_counter.most_common(max_vocab_size):\n",
    "            vocab[key] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "            \n",
    "    for key, value in vocab.items():\n",
    "        reverse_vocab[value] = key\n",
    "            \n",
    "    return vocab, reverse_vocab, max_vocab_size\n",
    "\n",
    "vocab,reverse_vocab,max_vocab_size = build_vocab(all_input_sentences+all_target_sentences)\n",
    "\n",
    "def token2idx(word, vocab):\n",
    "    return vocab[word]\n",
    "\n",
    "def sent2idx(sent, vocab=vocab, max_sentence_length=enc_sentence_length, is_target=False):\n",
    "    tokens = tokenizer(sent)\n",
    "    current_length = len(tokens)\n",
    "    pad_length = max_sentence_length - current_length\n",
    "    if is_target:\n",
    "        return [0] + [token2idx(token, vocab) for token in tokens] + [2] + [1] * (pad_length-1), current_length + 1\n",
    "    else:\n",
    "        return [token2idx(token, vocab) for token in tokens] + [1] * pad_length, current_length\n",
    "\n",
    "def idx2token(idx, reverse_vocab):\n",
    "    return reverse_vocab[idx]\n",
    "\n",
    "def idx2sent(indices, reverse_vocab=reverse_vocab):\n",
    "    return \" \".join([idx2token(idx, reverse_vocab) for idx in indices])\n",
    "\n",
    "# Enc Example\n",
    "print('Hi What is your name?')\n",
    "print(sent2idx('Hi What is your name?'))\n",
    "\n",
    "# Dec Example\n",
    "print('Hi this is Jaemin.')\n",
    "print(sent2idx('Hi this is Jaemin.', max_sentence_length=dec_sentence_length, is_target=True))\n",
    "\n",
    "idx2sent([0, 16, 41, 7, 36, 3, 2, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_optimizer(opt):\n",
    "    if opt == \"adam\":\n",
    "        optfn = tf.train.AdamOptimizer\n",
    "    elif opt == \"sgd\":\n",
    "        optfn = tf.train.GradientDescentOptimizer\n",
    "    else:\n",
    "        assert(False)\n",
    "    return optfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_rnn_cell(cell_name,dim_size, train_phase = True, keep_prob = 0.75):\n",
    "    if cell_name == \"gru\":\n",
    "        cell = tf.contrib.rnn.GRUCell(dim_size)\n",
    "    elif cell_name == \"lstm\":\n",
    "        cell = tf.contrib.rnn.LSTMCell(dim_size)\n",
    "    else:\n",
    "        cell = tf.contrib.rnn.BasicRNNCell(dim_size)\n",
    "    if train_phase and keep_prob < 1.0:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(\n",
    "              cell=cell,\n",
    "              input_keep_prob=keep_prob,\n",
    "              output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "def multi_rnn_cell(cell_name,dim_size,num_layers = 1, train_phase = True, keep_prob=0.75):\n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        cell = single_rnn_cell(cell_name,dim_size, train_phase, keep_prob)\n",
    "        cells.append(cell)\n",
    "    \n",
    "    if len(cells) > 1:\n",
    "        final_cell = tf.contrib.rnn.MultiRNNCell(cells=cells)\n",
    "    else:\n",
    "        final_cell = cells[0]\n",
    "    return final_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicS2SModel(object):\n",
    "    def __init__(self, vocab, batch_size = 2, dim_size=128, rnn_cell = 'gru', num_layers=2, max_gradient_norm=5.0, atten_size=30, \n",
    "                 learning_rate=0.001, learning_rate_decay_factor=0.98, dropout=0.2,max_inference_lenght=10,\n",
    "                 max_source_len = 10, max_target_len = 10,beam_size =3, optimizer=\"adam\", mode ='train',\n",
    "                 use_beam_search = False):\n",
    "        assert mode in ['train', 'inference']\n",
    "        self.start_token = vocab.get(_START_)\n",
    "        self.end_token = vocab.get(_END_)\n",
    "        self.train_phase = True if mode == 'train' else False\n",
    "        self.cell_name = rnn_cell\n",
    "        self.dim_size = dim_size\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.num_layers = num_layers\n",
    "        self.keep_prob_config = 1.0 - dropout\n",
    "        self.atten_size = atten_size\n",
    "        \n",
    "        # decoder\n",
    "        self.max_inference_lenght = max_inference_lenght\n",
    "        \n",
    "        # beam search\n",
    "        self.beam_size = beam_size\n",
    "        self.beam_search = use_beam_search\n",
    "        \n",
    "        # learning\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        # if we use beam search decoder, we need to specify the batch size and max source len\n",
    "        if self.beam_search:\n",
    "            self.batch_size = batch_size\n",
    "            self.source_tokens = tf.placeholder(tf.int32, shape=[batch_size, max_source_len])\n",
    "            self.source_length = tf.placeholder(tf.int32, shape=[batch_size,])\n",
    "        else:\n",
    "            self.source_tokens = tf.placeholder(tf.int32,shape=[None,None])\n",
    "            self.source_length = tf.placeholder(tf.int32,shape=[None,])\n",
    "            \n",
    "        if self.train_phase:\n",
    "            self.target_tokens = tf.placeholder(tf.int32, shape=[None, None])\n",
    "            self.target_length = tf.placeholder(tf.int32, shape=[None,])\n",
    "         \n",
    "        with tf.variable_scope(\"S2S\",initializer = tf.uniform_unit_scaling_initializer(1.0)):\n",
    "            self.setup_embeddings()\n",
    "            #self.setup_encoder()\n",
    "            self.setup_bidirection_encoder()\n",
    "            self.setup_attention_decoder()\n",
    "                \n",
    "        if self.train_phase:\n",
    "            opt = get_optimizer(optimizer)(self.learning_rate)\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.losses, params)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
    "            self.gradient_norm = tf.global_norm(gradients)\n",
    "            self.param_norm = tf.global_norm(params)\n",
    "            self.updates = opt.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "    \n",
    "    def setup_embeddings(self):\n",
    "        with tf.variable_scope(\"Embeddings\"):\n",
    "            with tf.device('/cpu:0'):\n",
    "                self.enc_emd = tf.get_variable(\"encode_embedding\", [self.vocab_size, self.dim_size])\n",
    "                self.dec_emd = tf.get_variable(\"decode_embedding\", [self.vocab_size, self.dim_size])\n",
    "                self.encoder_inputs = tf.nn.embedding_lookup(self.enc_emd, self.source_tokens)\n",
    "                if self.train_phase:\n",
    "                    self.decoder_inputs = tf.nn.embedding_lookup(self.dec_emd, self.target_tokens)\n",
    "    \n",
    "    def setup_encoder(self):\n",
    "        cell = multi_rnn_cell(self.cell_name,self.dim_size, self.num_layers, self.train_phase,self.keep_prob_config)\n",
    "        outputs,state = tf.nn.dynamic_rnn(cell,inputs=self.encoder_inputs,sequence_length=self.source_length,dtype=tf.float32)\n",
    "        self.encode_output = outputs\n",
    "        self.encode_state = state\n",
    "        # using the state of last layer of rnn as initial state\n",
    "        self.decode_initial_state = self.encode_state[-1]\n",
    "        \n",
    "    def setup_bidirection_encoder(self):\n",
    "        fw_cell = single_rnn_cell('gru',self.dim_size, train_phase=self.train_phase, keep_prob=self.keep_prob_config)\n",
    "        bw_cell = single_rnn_cell('gru',self.dim_size, train_phase=self.train_phase, keep_prob=self.keep_prob_config)\n",
    "        \n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            outputs,states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = fw_cell,\n",
    "                cell_bw = bw_cell,\n",
    "                dtype = tf.float32,\n",
    "                sequence_length = self.source_length,\n",
    "                inputs = self.encoder_inputs\n",
    "                )\n",
    "            outputs_concat = tf.concat(outputs, 2)\n",
    "        self.encode_output = outputs_concat\n",
    "        self.encode_state = states\n",
    "        \n",
    "        # use Dense layer to convert bi-direction state to decoder inital state\n",
    "        convert_layer = Dense(self.dim_size,dtype=tf.float32,name=\"bi_convert\")\n",
    "        self.decode_initial_state = convert_layer(tf.concat(self.encode_state,axis=1))\n",
    "        \n",
    "    def setup_training_decoder_layer(self):\n",
    "        max_dec_len = tf.reduce_max(self.target_length, name='max_dec_len')\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_inputs,self.target_length,name=\"training_helper\")\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell = self.dec_cell,\n",
    "            helper = training_helper,\n",
    "            initial_state = self.initial_state,\n",
    "            output_layer = self.output_layer\n",
    "        )\n",
    "        train_dec_outputs, train_dec_last_state,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            training_decoder,\n",
    "            output_time_major=False,\n",
    "            impute_finished=True,\n",
    "            maximum_iterations=max_dec_len)\n",
    "        \n",
    "        # logits: [batch_size x max_dec_len x vocab_size]\n",
    "        logits = tf.identity(train_dec_outputs.rnn_output, name='logits')\n",
    "\n",
    "        # targets: [batch_size x max_dec_len x vocab_size]\n",
    "        targets = tf.slice(self.target_tokens, [0, 0], [-1, max_dec_len], 'targets')\n",
    "\n",
    "        masks = tf.sequence_mask(self.target_length,max_dec_len,dtype=tf.float32,name=\"mask\")\n",
    "        self.losses = tf.contrib.seq2seq.sequence_loss(logits=logits,targets=targets,weights=masks,name=\"losses\")\n",
    "        \n",
    "        # prediction sample for validation\n",
    "        self.valid_predictions = tf.identity(train_dec_outputs.sample_id, name='valid_preds')\n",
    "    \n",
    "    def setup_inference_decoder_layer(self):\n",
    "        start_tokens = tf.tile(tf.constant([self.start_token],dtype=tf.int32),[self.batch_size])\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding=self.dec_emd,\n",
    "            start_tokens=start_tokens,\n",
    "            end_token=self.end_token)\n",
    "        \n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            cell = self.dec_cell,\n",
    "            helper = inference_helper, \n",
    "            initial_state=self.initial_state,\n",
    "            output_layer=self.output_layer)\n",
    "        \n",
    "        infer_dec_outputs, infer_dec_last_state,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    inference_decoder,\n",
    "                    output_time_major=False,\n",
    "                    impute_finished=True,\n",
    "                    maximum_iterations=self.max_inference_lenght)\n",
    "        # [batch_size x dec_sentence_length], tf.int32\n",
    "        self.predictions = tf.identity(infer_dec_outputs.sample_id, name='predictions')\n",
    "            \n",
    "    def setup_beam_search_decoder_layer(self):\n",
    "        start_tokens = tf.tile(tf.constant([self.start_token],dtype=tf.int32),[self.batch_size])\n",
    "        bsd = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=self.dec_cell,\n",
    "                    embedding=self.dec_emd,\n",
    "                    start_tokens= start_tokens,\n",
    "                    end_token=self.end_token,\n",
    "                    initial_state=self.initial_state,\n",
    "                    beam_width=self.beam_size,\n",
    "                    output_layer=self.output_layer,\n",
    "                    length_penalty_weight=0.0)\n",
    "        # final_outputs are instances of FinalBeamSearchDecoderOutput\n",
    "        final_outputs, final_state, final_sequence_lengths = tf.contrib.seq2seq.dynamic_decode(\n",
    "            bsd, \n",
    "            output_time_major=False,\n",
    "           # impute_finished=True,\n",
    "            maximum_iterations=self.max_inference_lenght\n",
    "        )\n",
    "        beam_predictions = final_outputs.predicted_ids\n",
    "        self.beam_predictions = tf.transpose(beam_predictions,perm=[0,2,1])\n",
    "        self.beam_prob = final_outputs.beam_search_decoder_output.scores\n",
    "        self.beam_ids = final_outputs.beam_search_decoder_output.predicted_ids\n",
    "        \n",
    "    def setup_attention_decoder(self):\n",
    "        #dec_cell = multi_rnn_cell('gru',self.dim_size,num_layers=self.num_layers, train_phase = self.train_phase, keep_prob=self.keep_prob_config)\n",
    "        dec_cell = [single_rnn_cell(self.cell_name,self.dim_size,self.train_phase,self.keep_prob_config) for i in range(self.num_layers)]\n",
    "        if self.beam_search:\n",
    "            memory = tf.contrib.seq2seq.tile_batch(self.encode_output,multiplier = self.beam_size)\n",
    "            memory_sequence_length = tf.contrib.seq2seq.tile_batch(self.source_length,multiplier = self.beam_size)\n",
    "        else:\n",
    "            memory = self.encode_output\n",
    "            memory_sequence_length = self.source_length\n",
    "            \n",
    "        attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n",
    "            num_units = self.atten_size,\n",
    "            memory = memory,\n",
    "            memory_sequence_length = memory_sequence_length,\n",
    "            name = \"BahdanauAttention\"\n",
    "        )\n",
    "        dec_cell[0] = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell=dec_cell[0],\n",
    "            attention_mechanism=attn_mech,\n",
    "            attention_layer_size=self.atten_size)\n",
    "        \n",
    "        if self.beam_search:\n",
    "            tile_state = tf.contrib.seq2seq.tile_batch(self.decode_initial_state,self.beam_size)\n",
    "            initial_state = [tile_state for i in range(self.num_layers)]\n",
    "            cell_state = dec_cell[0].zero_state(dtype=tf.float32,batch_size=self.batch_size*self.beam_size)\n",
    "            initial_state[0] = cell_state.clone(cell_state=initial_state[0])\n",
    "            self.initial_state = tuple(initial_state)\n",
    "        else:\n",
    "            # we use dynamic batch size\n",
    "            self.batch_size = tf.shape(self.encoder_inputs)[0]\n",
    "            initial_state = [self.decode_initial_state for i in range(self.num_layers)]\n",
    "            cell_state = dec_cell[0].zero_state(dtype=tf.float32, batch_size = self.batch_size)\n",
    "            initial_state[0] = cell_state.clone(cell_state=initial_state[0])\n",
    "            self.initial_state = tuple(initial_state)\n",
    "            \n",
    "        print(self.initial_state)\n",
    "        self.dec_cell = tf.contrib.rnn.MultiRNNCell(dec_cell)\n",
    "        self.output_layer = Dense(self.vocab_size,kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "        if self.train_phase:\n",
    "            self.setup_training_decoder_layer()\n",
    "        else:\n",
    "            if self.beam_search:\n",
    "                self.setup_beam_search_decoder_layer()\n",
    "            else:\n",
    "                self.setup_inference_decoder_layer()\n",
    "    \n",
    "    def train_one_step(self,sess,encode_input,encode_len,decode_input,decode_len):\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.source_tokens] = encode_input\n",
    "        feed_dict[self.source_length] = encode_len\n",
    "        feed_dict[self.target_tokens] = decode_input\n",
    "        feed_dict[self.target_length] = decode_len\n",
    "        valid_predictions,loss,_ = sess.run([self.valid_predictions,self.losses,self.updates],feed_dict=feed_dict)\n",
    "        return valid_predictions,loss\n",
    "    \n",
    "    def inference(self,sess,encode_input,encode_len):\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.source_tokens] = encode_input\n",
    "        feed_dict[self.source_length] = encode_len\n",
    "        if self.beam_search:\n",
    "            predictions,probs,ids = sess.run([self.beam_predictions,self.beam_prob,self.beam_ids],feed_dict=feed_dict)\n",
    "            return predictions,ids\n",
    "        else:\n",
    "            predictions = sess.run([self.predictions],feed_dict=feed_dict)\n",
    "            return predictions\n",
    "    \n",
    "    def save_model(self,sess,checkpoint_dir):\n",
    "        writer = tf.summary.FileWriter(checkpoint_dir, sess.graph)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        saver.save(sess,checkpoint_dir + \"model.ckpt\",global_step=self.global_step)\n",
    "        \n",
    "    def restore_model(self,sess,checkpoint_dir):\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        saver.restore(sess,tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AttentionWrapperState(cell_state=<tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>, attention=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_1:0' shape=(?, 30) dtype=float32>, time=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()), <tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>)\n",
      "(AttentionWrapperState(cell_state=<tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>, attention=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_1:0' shape=(?, 30) dtype=float32>, time=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()), <tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>)\n",
      "beam\n",
      "(AttentionWrapperState(cell_state=<tf.Tensor 'S2S/tile_batch_2/Reshape:0' shape=(?, 128) dtype=float32>, attention=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_1:0' shape=(6, 30) dtype=float32>, time=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_2:0' shape=(6, 10) dtype=float32>, alignment_history=()), <tf.Tensor 'S2S/tile_batch_2/Reshape:0' shape=(?, 128) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = BasicS2SModel(vocab=vocab,mode=\"train\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = BasicS2SModel(vocab=vocab,mode=\"inference\")\n",
    "\n",
    "print(\"beam\")\n",
    "tf.reset_default_graph()\n",
    "model = BasicS2SModel(vocab=vocab,mode=\"inference\",use_beam_search=True,beam_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AttentionWrapperState(cell_state=<tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>, attention=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_1:0' shape=(?, 30) dtype=float32>, time=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()), <tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>)\n",
      "start to run\n",
      "Epoch 0\n",
      "epoch loss:  15.4586901665\n",
      "Input: What do you want to drink?\n",
      "Prediction: What I I I\n",
      "Target: Beer please!\n",
      "Input: What is your favorite beer?\n",
      "Prediction: like this , ,\n",
      "Target: Leffe brown!\n",
      "Epoch 20\n",
      "epoch loss:  3.45742550492\n",
      "Input: What do you want to drink?\n",
      "Prediction: _GO_ Beer please !\n",
      "Target: Beer please!\n",
      "Input: What is your favorite beer?\n",
      "Prediction: _GO_ Leffe ! !\n",
      "Target: Leffe brown!\n",
      "Epoch 40\n",
      "epoch loss:  1.51330478489\n",
      "Input: What do you want to drink?\n",
      "Prediction: _GO_ Beer please !\n",
      "Target: Beer please!\n",
      "Input: What is your favorite beer?\n",
      "Prediction: _GO_ Leffe brown !\n",
      "Target: Leffe brown!\n",
      "Epoch 60\n",
      "epoch loss:  0.943925239146\n",
      "Input: What do you want to drink?\n",
      "Prediction: _GO_ Beer please !\n",
      "Target: Beer please!\n",
      "Input: What is your favorite beer?\n",
      "Prediction: _GO_ Leffe brown !\n",
      "Target: Leffe brown!\n",
      "Epoch 80\n",
      "epoch loss:  0.550355852582\n",
      "Input: What do you want to drink?\n",
      "Prediction: _GO_ Beer please !\n",
      "Target: Beer please!\n",
      "Input: What is your favorite beer?\n",
      "Prediction: _GO_ Leffe brown !\n",
      "Target: Leffe brown!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model = BasicS2SModel(vocab=vocab,num_layers=3)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"start to run\")\n",
    "    loss_history = []\n",
    "    for epoch in range(100):\n",
    "        all_preds = []\n",
    "        epoch_loss = 0\n",
    "        for input_batch, target_batch in zip(input_batches, target_batches):\n",
    "            input_batch_tokens = []\n",
    "            target_batch_tokens = []\n",
    "            enc_sentence_lengths = []\n",
    "            dec_sentence_lengths = []\n",
    "\n",
    "            for input_sent in input_batch:\n",
    "                tokens, sent_len = sent2idx(input_sent)\n",
    "                input_batch_tokens.append(tokens)\n",
    "                enc_sentence_lengths.append(sent_len)\n",
    "\n",
    "            for target_sent in target_batch:\n",
    "                tokens, sent_len = sent2idx(target_sent,\n",
    "                             vocab=vocab,\n",
    "                             max_sentence_length=dec_sentence_length,\n",
    "                             is_target=True)\n",
    "                target_batch_tokens.append(tokens)\n",
    "                dec_sentence_lengths.append(sent_len)\n",
    "            batch_preds, batch_loss = model.train_one_step(sess,input_batch_tokens,enc_sentence_lengths,target_batch_tokens,dec_sentence_lengths)\n",
    "            epoch_loss += batch_loss\n",
    "            all_preds.append(batch_preds)\n",
    "        loss_history.append(epoch_loss)\n",
    "        if epoch % 20 == 0:\n",
    "            print('Epoch', epoch)\n",
    "            print('epoch loss: ', epoch_loss )\n",
    "            for input_sent, target_sent, pred in zip(input_batch, target_batch, batch_preds):\n",
    "                print('Input:', input_sent)\n",
    "                print('Prediction:', idx2sent(pred, reverse_vocab=reverse_vocab))\n",
    "                print('Target:', target_sent)\n",
    "    writer = tf.summary.FileWriter('/tmp/test_s2s/', sess.graph)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    saver.save(sess,\"/tmp/test_s2s/model.ckpt\",global_step=model.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Gready search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AttentionWrapperState(cell_state=<tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>, attention=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_1:0' shape=(?, 30) dtype=float32>, time=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_2:0' shape=(?, ?) dtype=float32>, alignment_history=()), <tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'S2S/bi_convert/BiasAdd:0' shape=(?, 128) dtype=float32>)\n",
      "INFO:tensorflow:Restoring parameters from /tmp/test_s2s/model.ckpt-400\n",
      "Input: Which programming language do you use?\n",
      "[ 0  9 39 45  3  3  3  3  3  3]\n",
      "Prediction: _GO_ I like Python . . . . . .\n",
      "Target: I like Python.\n",
      "Input: See you later.\n",
      "[ 0 14 14  3  3  3  3  3  3  3]\n",
      "Prediction: _GO_ Bye Bye . . . . . . .\n",
      "Target: Bye Bye.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = BasicS2SModel(vocab=vocab,mode=\"inference\",use_beam_search=False,num_layers=3)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('/tmp/test_s2s/'))\n",
    "    batch_preds = []\n",
    "    batch_tokens = []\n",
    "    batch_sent_lens = []\n",
    "\n",
    "    for input_sent in input_batches[1]:\n",
    "        tokens, sent_len = sent2idx(input_sent)\n",
    "        batch_tokens.append(tokens)\n",
    "        batch_sent_lens.append(sent_len)\n",
    "\n",
    "    batch_preds = model.inference(sess,batch_tokens,batch_sent_lens)\n",
    "\n",
    "    for input_sent, target_sent, pred in zip(input_batches[1], target_batches[1], batch_preds[0]):\n",
    "        print('Input:', input_sent)\n",
    "        print(pred)\n",
    "        print('Prediction:', idx2sent(pred, reverse_vocab=reverse_vocab))\n",
    "        print('Target:', target_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Beam search\n",
    "\n",
    "> Beam search results are not good, need to take a deep look into the implementation, suggested not to use it right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(AttentionWrapperState(cell_state=<tf.Tensor 'S2S/tile_batch_2/Reshape:0' shape=(?, 128) dtype=float32>, attention=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_1:0' shape=(6, 30) dtype=float32>, time=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'S2S/AttentionWrapperZeroState/zeros_2:0' shape=(6, 10) dtype=float32>, alignment_history=()), <tf.Tensor 'S2S/tile_batch_2/Reshape:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'S2S/tile_batch_2/Reshape:0' shape=(?, 128) dtype=float32>)\n",
      "INFO:tensorflow:Restoring parameters from /tmp/test_s2s/model.ckpt-400\n",
      "Input: What do you want to drink?\n",
      "Prediction: _GO_ Beer please ! ! ! ! ! ! .\n",
      "Prediction: _GO_ Beer please ! ! ! ! ! . .\n",
      "Prediction: _GO_ Beer please ! ! ! ! ! ! !\n",
      "Target: Beer please!\n",
      "Input: What is your favorite beer?\n",
      "Prediction: _GO_ Leffe brown ! ! ! ! . . .\n",
      "Prediction: _GO_ Leffe brown ! ! ! . . . .\n",
      "Prediction: _GO_ Leffe brown ! ! ! too . . .\n",
      "Target: Leffe brown!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = BasicS2SModel(vocab=vocab,mode=\"inference\",use_beam_search=True,beam_size=3,num_layers=3)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('/tmp/test_s2s/'))\n",
    "    batch_preds = []\n",
    "    batch_tokens = []\n",
    "    batch_sent_lens = []\n",
    "\n",
    "    for input_sent in input_batches[3]:\n",
    "        tokens, sent_len = sent2idx(input_sent)\n",
    "        batch_tokens.append(tokens)\n",
    "        batch_sent_lens.append(sent_len)\n",
    "    batch_preds,ids = model.inference(sess,batch_tokens,batch_sent_lens)\n",
    "    for input_sent, target_sent, pred in zip(input_batches[3], target_batches[3], batch_preds):\n",
    "        print('Input:', input_sent)\n",
    "        for p in pred:\n",
    "            print('Prediction:', idx2sent(p, reverse_vocab=reverse_vocab))\n",
    "        print('Target:', target_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
